{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course: Introduction To GenAI\n",
    "\n",
    "*Notebook: Experimenting_with_LLMs.ipynb*\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/gassaf2/IntroductionToGenAI/blob/main/week3/Experimenting_with_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fI1offS40zi"
   },
   "source": [
    "# **Week 3 Hands-on Lab: Experimenting With Large Language Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff4LHilX5Kf9"
   },
   "source": [
    "**Introduction:**\n",
    "\n",
    "In this hands-on python notebook, we willl be experimenting with LLMs.\n",
    "This will help you:\n",
    "1.\tUse a pre-trained LLM from the Hugging Face library for text summarization.\n",
    "2.\tImplement a question-answering task using a pre-trained LLM.\n",
    "3.\tUnderstand how LLMs perform NLP tasks in real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDx0zQPu5rpm"
   },
   "source": [
    "# **Part 1: Text Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLvntDtm5y3K"
   },
   "source": [
    "**1. Import Necessary Libraries**\n",
    "\n",
    "We will be using the [Transformers](https://huggingface.co/docs/transformers/en/index) library from [Hugging Face](https://huggingface.co/). The Transformers library provides APIs and tools to easily download and train state-of-the-art pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PsDc_idx40Jt"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8RiZlIv6C6I"
   },
   "source": [
    "**2. Set up the Summarization Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rCq2G0236HgS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\gassaf\\.conda\\envs\\conda_lau310\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73edd910dc7a4da291ef94dbbe330299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gassaf\\.conda\\envs\\conda_lau310\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gassaf\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663337003005499f8380c7c7231ef5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ba6e6075ab414784c960e799ef7c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c0640406b84ea8ad422ad498bfa55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d987cfbd35443a8e8b0ccea4bfb30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " Artificial intelligence (AI) is rapidly transforming industries, from healthcare and education to finance and entertainment . Large Language Models (LLMs) are at the forefront of this transformation . These models are trained on vast datasets and can generate human-like\n"
     ]
    }
   ],
   "source": [
    "# Load the summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Input a long text for summarization\n",
    "long_text = \"\"\"\n",
    "Artificial intelligence (AI) is rapidly transforming industries, from healthcare and education to finance and entertainment.\n",
    "Generative AI models, such as Large Language Models (LLMs), are at the forefront of this transformation. These models are\n",
    "trained on vast datasets and can generate human-like text, enabling applications like automated customer support,\n",
    "personalized education tools, and content creation. Despite their potential, challenges such as bias, ethical concerns,\n",
    "and the environmental impact of large-scale training persist. Addressing these challenges is crucial for the responsible\n",
    "deployment of AI technologies in the future.\n",
    "\"\"\"\n",
    "\n",
    "# Generate a summary\n",
    "summary = summarizer(long_text, max_length=50, min_length=25, do_sample=False)\n",
    "print(\"Summary:\")\n",
    "print(summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rCq2G0236HgS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " The Great Fire of London destroyed 13,000 homes and numerous landmarks such as St. Paul’s Cathedral . The fire started in a bakery on Pudding Lane and quickly spread due to strong winds and the abundance of wooden structures .\n"
     ]
    }
   ],
   "source": [
    "# Load the summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Input a long text for summarization\n",
    "long_text = \"\"\"\n",
    "The Great Fire of London in 1666 was a catastrophic event that destroyed much of the city, including over 13,000 homes and numerous \n",
    "landmarks such as St. Paul’s Cathedral. The fire started in a bakery on Pudding Lane and quickly spread due to strong winds and the\n",
    "abundance of wooden structures. Although the official death toll was recorded as low, recent studies suggest that many undocumented\n",
    "casualties occurred, particularly among the city’s poorer residents. In the aftermath, King Charles II mandated a complete ban on\n",
    "wooden buildings, replacing them with fire-resistant stone structures. Interestingly, some historians argue that the fire was \n",
    "deliberately set to cover up a failed political coup, a theory that remains a topic of debate today.\n",
    "\"\"\"\n",
    "\n",
    "# Generate a summary\n",
    "summary = summarizer(long_text, max_length=50, min_length=25, do_sample=False)\n",
    "print(\"Summary:\")\n",
    "print(summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiFUwWrZ6Sa9"
   },
   "source": [
    "**3. Experiment**\n",
    "\n",
    "* Replace long_text with any article or paragraph of your choice.\n",
    "* Try different max_length and min_length values to see how they affect the summary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24nKFnb56mhi"
   },
   "source": [
    "# **Part 2: Question Answering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHIEB48L6qQ1"
   },
   "source": [
    "**1.\tSet Up the Question-Answering Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xbFCfzNi6V3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8dc2f8b8b7440e7aae1392bd22f0b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gassaf\\.conda\\envs\\conda_lau310\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gassaf\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24bf7a7e2114ac1af317e0dbb7e29a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f766e9a4a7194a06a2bf7814ce424e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0c1cf590b843e39718370ee65141ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee8acc793054d1490017c1507cda772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Transformer\n"
     ]
    }
   ],
   "source": [
    "# Load the question-answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "# Input context and questions\n",
    "context = \"\"\"\n",
    "The Large Language Model (LLM) GPT-3, developed by OpenAI, is known for its exceptional ability to generate human-like text.\n",
    "It uses the Transformer architecture and has 175 billion parameters, making it one of the largest AI models in the world.\n",
    "LLMs like GPT-3 are widely used in applications such as content creation, summarization, and question answering.\n",
    "\"\"\"\n",
    "\n",
    "question = \"What architecture does GPT-3 use?\"\n",
    "\n",
    "# Get the answer\n",
    "answer = qa_pipeline(question=question, context=context)\n",
    "print(\"Answer:\")\n",
    "print(answer['answer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Otq9wqZc6y93"
   },
   "source": [
    "**2.\tExperiment**\n",
    "\n",
    "* Modify the context and question variables with your own text and queries.\n",
    "* Observe how the model adjusts its answers based on the provided input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "negGMYGt64tG"
   },
   "source": [
    "# **Part 3: Combine Summarization and Question Answering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OrgCwu669OV"
   },
   "source": [
    "**1. Pipeline Integration**\n",
    "\n",
    "Combine the two pipelines to first summarize a long text and then extract answers to specific questions from the summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oM6KZs_V7F0v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the challenges mentioned in the summary?\n",
      "Answer: These models are trained on vast datasets\n"
     ]
    }
   ],
   "source": [
    "# Summarize the text\n",
    "summary = summarizer(long_text, max_length=50, min_length=25, do_sample=False)[0]['summary_text']\n",
    "\n",
    "# Define a question based on the summary\n",
    "question = \"What are the challenges mentioned in the summary?\"\n",
    "\n",
    "# Use the QA pipeline to extract the answer\n",
    "answer = qa_pipeline(question=question, context=summary)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer['answer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAyjeci67JVX"
   },
   "source": [
    "**2. Experiment**\n",
    "\n",
    "•\tChange the input text and questions to test the robustness of the combined approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1pbj-m07hyw"
   },
   "source": [
    "# **Summary:**\n",
    "\n",
    "By completing this activity, you have:\n",
    "\n",
    "* Gained hands-on experience using LLMs for real-world NLP tasks.\n",
    "* Understood the capabilities and limitations of pre-trained LLMs.\n",
    "* Appreciated the practical applications of LLMs in summarization and question answering.\n",
    "\n",
    "This activity ensures practical understanding of LLMs while showcasing their real-world relevance. Let me know if you’d like additional extensions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
