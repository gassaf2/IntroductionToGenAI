{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course: Introduction To GenAI\n",
    "\n",
    "*Notebook: Building_a_Simplified_Transformer_Encoder.ipynb*\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/gassaf2/IntroductionToGenAI/blob/main/week3/Building_a_Simplified_Transformer_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvODY4XYGS4s"
   },
   "source": [
    "# **Week 3 Hands-on Lab: Building a Simplified Transformer Encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tV7jIspzGcIM"
   },
   "source": [
    "This hands-on lab allows you to understand the Transformer architecture by implementing a basic Transformer encoder. You will learn how input embeddings, positional encodings, and feedforward layers work together in an encoder block. We will be using the Torch framework to build a simple transformer encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UD0bA42xGnZA"
   },
   "source": [
    "# **Part 1: Input Embedding and Positional Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-uZW8_bGvUJ"
   },
   "source": [
    "**1.\tGenerate Input Data**\n",
    "Define a sample sentence and tokenize it into a numerical format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "bulopUQ1GzrC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Example sentence and token IDs (simplified for illustration)\n",
    "token_ids = torch.tensor([[1, 2, 3, 4, 5]])  # Tokenized sentence\n",
    "vocab_size = 10  # Vocabulary size\n",
    "embedding_dim = 8  # Embedding size\n",
    "\n",
    "\n",
    "## Georges Assaf. modifying the tensor and the variable vocab_size and embedding_dim\n",
    "token_ids = torch.tensor([[1, 2, 3, 4, 5,6]])  # Tokenized sentence\n",
    "vocab_size = 20  # Vocabulary size\n",
    "embedding_dim = 16  # Embedding size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4khCooiqIdHf"
   },
   "source": [
    "**2. Create an Embedding Layer**\n",
    "Implement the embedding layer to convert token IDs into dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "LjWIeI7GIoEp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Tokens:\n",
      " tensor([[[ 1.2946, -1.6176, -1.4281, -0.3603,  0.3961, -0.5066, -0.8373,\n",
      "           0.0259, -0.1213, -0.8311,  0.0902, -0.4314, -0.9263,  0.1306,\n",
      "           0.3347, -1.4473],\n",
      "         [ 0.2507,  0.0974,  0.0320,  0.1344, -0.0562,  1.7124,  0.2595,\n",
      "          -0.6400,  1.0983,  1.5185,  1.6119, -0.4933,  2.1423,  0.6859,\n",
      "           1.7850, -2.0000],\n",
      "         [ 1.0554, -2.2970,  0.2895,  0.5738, -1.5454,  0.4678,  0.6180,\n",
      "           0.3973, -0.9604,  1.2303,  0.3447, -0.9984,  1.9419, -0.2734,\n",
      "           1.9376, -2.0838],\n",
      "         [-0.3119,  1.6137, -0.4106, -0.3783,  0.7321, -0.1668, -1.2336,\n",
      "           0.9647, -0.3289,  0.0798,  0.5524,  0.0797,  1.1040,  1.2905,\n",
      "          -1.2435, -0.0028],\n",
      "         [-0.1095,  1.1084,  0.3514, -0.4367,  1.5886,  0.4718,  0.1962,\n",
      "           2.0646,  0.9635, -0.4830, -1.0385, -0.6251,  0.1583,  0.2104,\n",
      "          -0.2026, -0.5687],\n",
      "         [-0.3105, -0.4894,  0.7193, -1.4652, -0.0137, -0.4502,  0.6476,\n",
      "           0.8255,  0.2186, -0.2603, -0.3297, -0.6661, -0.1621,  0.1182,\n",
      "           0.8955,  0.0178]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "embedded_tokens = embedding_layer(token_ids)\n",
    "print(\"Embedded Tokens:\\n\", embedded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfdPOMcpG_qJ"
   },
   "source": [
    "**3.\tAdd Positional Encoding**\n",
    "Incorporate positional encoding to provide positional information to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Mti1h0tXHDXY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding:\n",
      " tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  3.1098e-01,  9.5042e-01,  9.9833e-02,\n",
      "          9.9500e-01,  3.1618e-02,  9.9950e-01,  9.9998e-03,  9.9995e-01,\n",
      "          3.1623e-03,  9.9999e-01,  1.0000e-03,  1.0000e+00,  3.1623e-04,\n",
      "          1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  5.9113e-01,  8.0658e-01,  1.9867e-01,\n",
      "          9.8007e-01,  6.3203e-02,  9.9800e-01,  1.9999e-02,  9.9980e-01,\n",
      "          6.3245e-03,  9.9998e-01,  2.0000e-03,  1.0000e+00,  6.3246e-04,\n",
      "          1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  8.1265e-01,  5.8275e-01,  2.9552e-01,\n",
      "          9.5534e-01,  9.4726e-02,  9.9550e-01,  2.9996e-02,  9.9955e-01,\n",
      "          9.4867e-03,  9.9995e-01,  3.0000e-03,  1.0000e+00,  9.4868e-04,\n",
      "          1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  9.5358e-01,  3.0114e-01,  3.8942e-01,\n",
      "          9.2106e-01,  1.2615e-01,  9.9201e-01,  3.9989e-02,  9.9920e-01,\n",
      "          1.2649e-02,  9.9992e-01,  4.0000e-03,  9.9999e-01,  1.2649e-03,\n",
      "          1.0000e+00],\n",
      "        [-9.5892e-01,  2.8366e-01,  9.9995e-01, -1.0342e-02,  4.7943e-01,\n",
      "          8.7758e-01,  1.5746e-01,  9.8753e-01,  4.9979e-02,  9.9875e-01,\n",
      "          1.5811e-02,  9.9988e-01,  5.0000e-03,  9.9999e-01,  1.5811e-03,\n",
      "          1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "def positional_encoding(seq_len, embedding_dim):\n",
    "    position = np.arange(seq_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, embedding_dim, 2) * -(np.log(10000.0) / embedding_dim))\n",
    "    pe = np.zeros((seq_len, embedding_dim))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return torch.tensor(pe, dtype=torch.float)\n",
    "\n",
    "seq_len = token_ids.size(1)\n",
    "pos_encoding = positional_encoding(seq_len, embedding_dim)\n",
    "print(\"Positional Encoding:\\n\", pos_encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZm2hKp-HIxR"
   },
   "source": [
    "Add the positional encoding to the embedded tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "TMwXiAIiHLm7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Tokens with Positional Encoding:\n",
      " tensor([[[ 1.2946e+00, -6.1765e-01, -1.4281e+00,  6.3972e-01,  3.9609e-01,\n",
      "           4.9344e-01, -8.3733e-01,  1.0259e+00, -1.2126e-01,  1.6891e-01,\n",
      "           9.0194e-02,  5.6857e-01, -9.2628e-01,  1.1306e+00,  3.3474e-01,\n",
      "          -4.4732e-01],\n",
      "         [ 1.0921e+00,  6.3766e-01,  3.4296e-01,  1.0848e+00,  4.3619e-02,\n",
      "           2.7074e+00,  2.9112e-01,  3.5952e-01,  1.1083e+00,  2.5184e+00,\n",
      "           1.6151e+00,  5.0674e-01,  2.1433e+00,  1.6859e+00,  1.7853e+00,\n",
      "          -1.0000e+00],\n",
      "         [ 1.9647e+00, -2.7131e+00,  8.8061e-01,  1.3804e+00, -1.3467e+00,\n",
      "           1.4479e+00,  6.8123e-01,  1.3953e+00, -9.4042e-01,  2.2301e+00,\n",
      "           3.5104e-01,  1.5669e-03,  1.9439e+00,  7.2657e-01,  1.9382e+00,\n",
      "          -1.0838e+00],\n",
      "         [-1.7074e-01,  6.2370e-01,  4.0207e-01,  2.0448e-01,  1.0277e+00,\n",
      "           7.8855e-01, -1.1389e+00,  1.9602e+00, -2.9895e-01,  1.0793e+00,\n",
      "           5.6191e-01,  1.0797e+00,  1.1070e+00,  2.2905e+00, -1.2426e+00,\n",
      "           9.9720e-01],\n",
      "         [-8.6627e-01,  4.5476e-01,  1.3050e+00, -1.3552e-01,  1.9781e+00,\n",
      "           1.3929e+00,  3.2232e-01,  3.0566e+00,  1.0035e+00,  5.1622e-01,\n",
      "          -1.0258e+00,  3.7478e-01,  1.6231e-01,  1.2103e+00, -2.0134e-01,\n",
      "           4.3134e-01],\n",
      "         [-1.2694e+00, -2.0570e-01,  1.7192e+00, -1.4756e+00,  4.6574e-01,\n",
      "           4.2737e-01,  8.0502e-01,  1.8130e+00,  2.6856e-01,  7.3844e-01,\n",
      "          -3.1392e-01,  3.3377e-01, -1.5713e-01,  1.1182e+00,  8.9709e-01,\n",
      "           1.0178e+00]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedded_with_pos = embedded_tokens + pos_encoding.unsqueeze(0)\n",
    "print(\"Embedded Tokens with Positional Encoding:\\n\", embedded_with_pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfM3VX60HQ6b"
   },
   "source": [
    "# **Part 2: Add a Feedforward Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUaKhCEuHViQ"
   },
   "source": [
    "1.\t**Define a Feedforward Neural Network**\n",
    "Implement a simple feedforward layer as part of the encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "RIN1MxZVHZwv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward Output:\n",
      " tensor([[[-0.1572, -0.2207, -0.4988, -0.2427, -0.4057, -0.2053,  0.4138,\n",
      "           0.1484, -0.2072,  0.4187, -0.1409,  0.3280, -0.0890,  0.1992,\n",
      "          -0.0903, -0.5097],\n",
      "         [-0.1450,  0.2609, -0.6227, -0.0029, -0.5559,  0.2653, -0.0430,\n",
      "           0.1175, -0.2884,  0.4779, -0.5457,  0.1647,  0.2225,  0.6340,\n",
      "          -0.0211, -0.7522],\n",
      "         [-0.2102,  0.0908, -0.6323, -0.3001, -0.7394, -0.1145,  0.1823,\n",
      "           0.0314, -0.5904,  0.5787, -0.5822,  0.3015,  0.2708,  0.4998,\n",
      "          -0.0301, -0.9231],\n",
      "         [ 0.0528, -0.0779, -0.2815, -0.0507, -0.1358,  0.1043,  0.2363,\n",
      "          -0.0069,  0.0647,  0.2521, -0.1855,  0.3481, -0.1037,  0.0311,\n",
      "          -0.0835, -0.3319],\n",
      "         [ 0.0192, -0.0683, -0.1524,  0.0207, -0.2571, -0.0028,  0.1125,\n",
      "           0.0142,  0.0444,  0.3107, -0.3062,  0.2579, -0.0363, -0.0447,\n",
      "          -0.0898, -0.4226],\n",
      "         [ 0.2295,  0.0241, -0.0819,  0.0567, -0.1269, -0.0971, -0.0043,\n",
      "           0.1104,  0.0459,  0.3016, -0.3280,  0.0660,  0.0881, -0.2442,\n",
      "          -0.2073, -0.5029]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "feedforward = nn.Sequential(\n",
    "    nn.Linear(embedding_dim, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, embedding_dim)\n",
    ")\n",
    "ff_output = feedforward(embedded_with_pos)\n",
    "print(\"Feedforward Output:\\n\", ff_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xb9uak0OHcjg"
   },
   "source": [
    "# **Part 3: Combine the Components into an Encoder Block**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXVdYvwkHgdL"
   },
   "source": [
    "1.\t**Define the Encoder Block**\n",
    "Combine the embedding, positional encoding, and feedforward components into an encoder block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "70IMzzkPHmRH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output:\n",
      " tensor([[[ 1.3294e-01,  1.4697e+00, -6.7457e-01, -2.7197e-01, -1.8065e-03,\n",
      "           1.5336e+00, -2.2453e+00,  7.3532e-01, -4.6048e-01, -5.3451e-01,\n",
      "          -5.0691e-02,  1.3836e+00,  1.7041e-01, -1.5720e+00, -2.3713e-01,\n",
      "           6.2297e-01],\n",
      "         [ 7.7386e-01,  1.2560e+00, -6.5581e-02,  6.4131e-01, -4.5938e-01,\n",
      "           1.5621e+00, -4.6505e-01,  5.0597e-01, -2.7963e+00, -7.2114e-01,\n",
      "          -5.6513e-01,  6.3266e-01,  2.1282e-02,  7.1487e-01, -2.1978e-01,\n",
      "          -8.1577e-01],\n",
      "         [ 7.2185e-01, -1.0716e+00, -2.4328e-01, -2.7066e-01,  8.2456e-01,\n",
      "           1.0211e+00, -1.4951e+00,  1.4192e-01, -9.2208e-01,  9.3943e-01,\n",
      "          -1.8538e+00,  1.4610e+00, -8.3058e-01, -2.6823e-02,  1.4873e+00,\n",
      "           1.1674e-01],\n",
      "         [-1.8008e+00, -1.3058e+00,  5.2797e-01,  1.0503e+00,  2.6161e-01,\n",
      "           3.7693e-01,  2.7536e-01,  1.1176e+00,  1.4152e-02,  1.8708e+00,\n",
      "           4.7820e-01, -1.9664e+00, -7.2031e-01, -4.9409e-02,  3.6081e-03,\n",
      "          -1.3372e-01],\n",
      "         [ 3.1331e-01, -6.1682e-01,  1.0336e+00,  1.2039e+00,  9.3711e-01,\n",
      "           7.9246e-01,  2.8236e-01,  1.5581e+00, -4.8967e-01,  2.3328e-01,\n",
      "          -3.6672e-01,  5.0920e-01, -1.7015e+00, -1.1096e+00, -1.9414e+00,\n",
      "          -6.3762e-01],\n",
      "         [-2.0532e+00,  8.1737e-04,  1.1615e+00,  1.7092e-01, -3.3863e-02,\n",
      "           1.0246e+00,  6.4843e-01,  1.8194e+00, -1.2194e+00,  5.2759e-01,\n",
      "           1.2975e-01,  4.6387e-02, -1.4904e+00,  2.3101e-01, -1.2239e+00,\n",
      "           2.6030e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, embedding_dim)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        pos_enc = positional_encoding(x.size(1), embed.size(2))\n",
    "        embed_with_pos = embed + pos_enc.unsqueeze(0)\n",
    "        ff_output = self.feedforward(embed_with_pos)\n",
    "        return self.layer_norm(embed_with_pos + ff_output)\n",
    "\n",
    "encoder = TransformerEncoderBlock(vocab_size, embedding_dim)\n",
    "output = encoder(token_ids)\n",
    "print(\"Encoder Output:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfB-L5zlHqhV"
   },
   "source": [
    "**Part 4: Experiment with Different Inputs**\n",
    "\n",
    "* Test with Different Sentences\n",
    "Replace token_ids with new examples to observe how the encoder processes different inputs.\n",
    "* Modify Hyperparameters\n",
    "Experiment with different embedding sizes, feedforward dimensions, or positional encoding scales to see their effect on the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZW4m09oIIFm"
   },
   "source": [
    "# **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49Ko4E9cH4df"
   },
   "source": [
    "By completing this lab, you have:\n",
    "\n",
    "* Understood the role of embedding, positional encoding, and feedforward layers in the Transformer encoder.\n",
    "* Gained hands-on experience implementing a core component of the Transformer architecture.\n",
    "* Developed a deeper appreciation for the architecture’s design and functionality.\n",
    "\n",
    "This lab builds foundational knowledge of the Transformer, preparing you for more advanced concepts like self-attention.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
